# The report!

### Name
Joel Bäcker (jo4383ba-s)

### Introduction

In this computer exercise we will take a look at different classification problems where different shapes will be separated. In addition to that the concept of a pulse converter will be explore, where we train a model on a certain wave shape and then try to predict a new one. In the last part of the lab a network will try to create code by training on Tensorflow's source code.

### Answers to questions

1. The performance of the CNN were:

```
validation: Accuracy = 0.9935, Sensitivity = 0.9906, Specificity = 0.9966
```

2. I could shrink the system to only consisting of 1 (instead of 10) node in the MLP and the second 2D kernel to 3 from 4. Why this can be done is perhaps because when there are few and quite distinct classes ('5' and '6'), a larger network does not help to separate the two.

The number of trainable parameters is 229.

3. The networks tested consists of 2 layers with different number of kernels in the first layer. In the first layer it seems to preserve some aspects of the image while the second detect more abstract features. Using only one kernel in the first layer seemed to find the edges of the numbers. When three kernels were used (as in the suggested model) they seemed to identify either edges from certain directions or the shape itself.

4. The performance of the model where the following

```
Training  log_loss:   0.08883408893799362
Training  accuracy:   0.978

Validation  log_loss:   0.1327794520912242
Validation  accuracy:   0.951
```

and the model (that have been changed)

```

# First conv layer
ex3.add(Conv2D(3, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Second conv layer
ex3.add(Conv2D(3, kernel_size=(3, 3), activation='relu'))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Third conv layer
ex3.add(Conv2D(2, kernel_size=(2, 2), activation='relu'))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Fully connected MLP layers
ex3.add(Flatten())
ex3.add(Dense(10, activation='relu'))

# Output layer
ex3.add(Dense(3, activation='softmax'))

# We use cross entropy error and the adam optimizer
adam = Adam(lr=0.003)
ex3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
ex3.summary()

# Now train the ex3
estimator_ex3 = ex3.fit(x_trn, d_trn,
                      validation_data=(x_val, d_val),
                      epochs=15,
                      batch_size=40,
                      verbose=0)
```

5. The images that are supposed to be classified in this problem are from R3 which consists of rectangles, squares, "horizontal" rectangles and "vertical" rectangles. During the previous question we had to considered circles, rectangles and triangles. That is why this is a harder problem, in this question we consider shapes that are all of 4 corners and thus similar, while in question 4 the shapes were 3 distinct ones.

 My performance and model to classify the data for R3 is seen below.

```
Training  log_loss:   0.39492630531441
Training  accuracy:   0.956

Validation  log_loss:   1.050755587380374
Validation  accuracy:   0.706
```

```

# First conv layer
ex3.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Second conv layer
ex3.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Third conv layer
ex3.add(Conv2D(2, kernel_size=(2, 2), activation='relu'))
ex3.add(MaxPooling2D(pool_size=(2, 2)))

# Fully connected MLP layers
ex3.add(Flatten())
ex3.add(Dense(20, activation='relu'))

# Output layer
ex3.add(Dense(3, activation='softmax'))

# We use cross entropy error and the adam optimizer
adam = Adam(lr=0.003)
ex3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
ex3.summary()

# Now train the ex3
estimator_ex3 = ex3.fit(x_trn, d_trn,
                      validation_data=(x_val, d_val),
                      epochs=15,
                      batch_size=40,
                      verbose=0)
```

6. The average validation error of a simpleRNN (with 5 hidden nodes and 20 epochs) system were 0.23444.

7. To figure out which of the different systems performed the best with roughly the same parameters the average of 3 runs of the validation error are compared. To adjust the number of parameters in each system the variable ´nh1´ is changed. The result and number of trainable parameters is seen below:

|                      | simpleRNN | GRU     | LSTM    |
| :------------------- | :-------- | :------ | :------ |
| Parameters           | 177       | 169     | 199     |
| Avg validation error | 0.11392   | 0.02975 | 0.03629 |

8. My guess of what the different hidden nodes are detecting is one finds the length of a pulse in the square wave and the other two is combined in some way to calculate the level of that pulse.

9. Why this problem is much harder than the previous one could be because of the fast changes in the square waves that are not present in triangular waves. So to compensate for this more nodes are needed.

tidigare så kombineras två för att skapa höjd, men hur ser det ut för trianglar

```
# The network type
net = SimpleRNN
# net = GRU
# net = LSTM

# Number of hidden nodes
nh1 = 40

# This is only if you would like to add an additional hidden layer. See below.
nh2 = 20

# The activation function
activation = 'tanh'
# activation = 'relu'

# The number of epochs
nE = 30

adam = Adam(lr=0.003)
```

10. Yes (I guess, not overly familiar with C++).

11. If the `temp` variable is increase, more longer words are created.

### Summary

In the first part of the lab we looked at a simple CNN, it was found that a few nodes could be cut when there were only a few classes. It was also found that in this case the CNN used edge detection to differentiate the two classes.

After that a harder problem were tried, when different shapes would be classified. When there were three distinct (triangles, circles and squares) it were easier to classify the shapes than when only different squares and rectangles were used. The last part of this problem required a larges network.

We also looked at pulse prediction were a wave is used as training and then a different wave is tried to predict. It was found that going from square to triangle waves were much easier than the opposite, maybe because of the fast changes in the square wave when it were the training data.

Lastly, a RNN were used to sample characters to mimic that of code. It created code that seemed to be correct and the length of the words could be adjusted using the variable `temp`.
